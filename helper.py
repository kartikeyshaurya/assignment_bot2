import pandas as pd
from elasticsearch import Elasticsearch, helpers
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np


from elasticsearch import Elasticsearch

# Password for the 'elastic' user generated by Elasticsearch
#!docker cp elastic:/usr/share/elasticsearch/config/certs/http_ca.crt .   for retrieving the CA certificate
ELASTIC_PASSWORD = "NAuf97gWR2bEPiI2F*rq"

# Create the client instance
client = Elasticsearch(
    "https://localhost:9200",
    ca_certs="http_ca.crt",
    basic_auth=("elastic", ELASTIC_PASSWORD)
)

es = client
index_name = "disease_prediction1"
# Successful response!
client.info()
# {'name': 'instance-0000000000', 'cluster_name': ...}

def load_model():
    model_name = "sentence-transformers/all-MiniLM-L6-v2"  # Change to the model of your choice
    model = AutoModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer


# Function to generate embeddings for symptoms
def generate_embeddings(texts, model, tokenizer):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings
    return embeddings.numpy()



# Function to index documents (diseases and Combined_Symptoms) with embeddings
def index_documents(df):
    model, tokenizer = load_model()

    symptoms = df['Combined_Symptoms'].tolist()
    embeddings = generate_embeddings(symptoms, model, tokenizer)
    
    actions = []
    for i, (disease, symptom, embedding) in enumerate(zip(df['Disease'], symptoms, embeddings)):
        action = {
            "_op_type": "index",
            "_index": index_name,
            "_id": i,  # Optional, you can set a custom ID or let Elasticsearch auto-generate
            "_source": {
                "disease": disease,
                "symptoms": symptom,
                "embedding": embedding.tolist()  # Convert to list for JSON serialization
            }
        }
        actions.append(action)

    print(actions)
    helpers.bulk(es, actions)
    print(f"Indexed {len(df)} documents.")


# Function to index documents (diseases and symptoms) with embeddings
def index_documents(df):
    model, tokenizer = load_model()

    symptoms = df['Combined_Symptoms'].tolist()
    embeddings = generate_embeddings(symptoms, model, tokenizer)
    
    actions = []
    for i, (disease, symptom, embedding) in enumerate(zip(df['Disease'], symptoms, embeddings)):
        action = {
            "_op_type": "index",
            "_index": index_name,
            "_id": i,  # Optional, you can set a custom ID or let Elasticsearch auto-generate
            "_source": {
                "disease": disease,
                "symptoms": symptom,
                "embedding": embedding.tolist()  # Convert to list for JSON serialization
            }
        }
        actions.append(action)

    # Bulk indexing with error handling
    try:
        response = helpers.bulk(es, actions)
        print(f"Successfully indexed {len(actions)} documents.")
    except helpers.BulkIndexError as e:
        print(f"BulkIndexError: {len(e.errors)} documents failed to index.")
        for error in e.errors:
            print(error)



# Function to search for symptoms based on user input
def search_symptoms(query, top_k=3):
    model, tokenizer = load_model()
    
    # Generate the embedding for the user query
    query_embedding = generate_embeddings([query], model, tokenizer)[0]

    # Search for the most similar symptoms in Elasticsearch
    script_query = {
        "script_score": {
            "query": {
                "match_all": {}  # Match all documents
            },
            "script": {
                "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                "params": {
                    "query_vector": query_embedding.tolist()
                }
            }
        }
    }
    
    # Perform the search and get the top-k results
    response = es.search(index=index_name, body={
        "size": top_k,
        "query": script_query
    })

    # Parse the response
    results = []
    for hit in response['hits']['hits']:
        score = hit['_score']
        disease = hit['_source']['disease']
        symptoms = hit['_source']['symptoms']
        results.append({"disease": disease, "symptoms": symptoms, "score": score})

    return results