{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a05263",
   "metadata": {},
   "source": [
    "## Preprocessing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed68c6",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2708907377.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    - The dataset is loaded from a CSV file (`dataset.csv`).\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Documentation\n",
    "\n",
    "- This jupyter Notebook is designed to preprocess a dataset of diseases and their associated symptoms, generate embeddings for the symptoms using a pre-trained transformer model, and index the data into Elasticsearch for efficient search and retrieval. Below is a summary of the key steps and functionalities implemented in this notebook:\n",
    "\n",
    "### 1. Preprocessing the Dataset\n",
    "- The dataset is loaded from a CSV file (`dataset.csv`).\n",
    "- A new column `Combined_Symptoms` is created by combining all symptom-related columns into a single string.\n",
    "- The dataset is filtered to retain only the `Disease` and `Combined_Symptoms` columns.\n",
    "- The data is grouped by `Disease`, and unique symptoms are aggregated for each disease.\n",
    "- The processed data is saved to a new CSV file (`disease_symptoms.csv`).\n",
    "\n",
    "### 2. Elasticsearch Setup\n",
    "- Elasticsearch is configured with a secure connection using a password and CA certificate.\n",
    "- An index (`disease_prediction1`) is created in Elasticsearch with mappings for `disease`, `symptoms`, and `embedding` fields. The `embedding` field is defined as a dense vector with 384 dimensions.\n",
    "\n",
    "### 3. Embedding Generation\n",
    "- A pre-trained transformer model (`sentence-transformers/all-MiniLM-L6-v2`) is used to generate embeddings for the symptoms.\n",
    "- Mean pooling is applied to the token embeddings to create a single vector representation for each symptom.\n",
    "\n",
    "### 4. Indexing Data into Elasticsearch\n",
    "- The processed dataset is indexed into Elasticsearch.\n",
    "- Each document contains the disease name, combined symptoms, and the embedding vector.\n",
    "- Bulk indexing is performed with error handling to ensure robustness.\n",
    "\n",
    "### 5. Searching for Symptoms\n",
    "- A search function is implemented to find the most relevant diseases based on user input symptoms.\n",
    "- The function generates an embedding for the user query and performs a cosine similarity search in Elasticsearch.\n",
    "- The top-k most relevant results are returned, including the disease name, symptoms, and relevance score.\n",
    "\n",
    "### 6. Dependencies\n",
    "- The notebook uses the following Python libraries:\n",
    "    - `pandas` for data manipulation.\n",
    "    - `transformers` for loading the pre-trained model and tokenizer.\n",
    "    - `torch` for tensor operations.\n",
    "    - `elasticsearch` for interacting with the Elasticsearch instance.\n",
    "    - `langchain-elasticsearch` for advanced Elasticsearch functionalities.\n",
    "\n",
    "### 7. Usage\n",
    "- Run the cells sequentially to preprocess the data, set up Elasticsearch, index the data, and perform searches.\n",
    "- Modify the `user_query` variable to test the search functionality with different symptoms.\n",
    "\n",
    "This notebook provides a complete pipeline for disease prediction based on symptoms using Elasticsearch and transformer-based embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee400f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506cbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Combined_Symptoms'] = df.apply(lambda row: ', '.join([str(val) for val in row if pd.notnull(val)]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Disease\",\"Combined_Combined_Symptoms\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Disease.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df.groupby('Disease')['Combined_Symptoms'].apply(lambda x: ', '.join(x.unique())).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae040c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checkpointers \n",
    "df_combined.to_csv(\"disease_symptoms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdddc69",
   "metadata": {},
   "source": [
    "## elasticsearch feeding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"disease_symptoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47841bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Password for the 'elastic' user generated by Elasticsearch\n",
    "#!docker cp elastic:/usr/share/elasticsearch/config/certs/http_ca.crt .   for retrieving the CA certificate\n",
    "ELASTIC_PASSWORD = \"NAuf97gWR2bEPiI2F*rq\"\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", ELASTIC_PASSWORD)\n",
    ")\n",
    "\n",
    "# Successful response!\n",
    "client.info()\n",
    "# {'name': 'instance-0000000000', 'cluster_name': ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-elasticsearch langchain-community tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d11095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_elasticsearch import SparseVectorStrategy\n",
    "from elasticsearch import Elasticsearch, exceptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76947f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index name for symptoms\n",
    "index_name = 'disease_prediction1'\n",
    "\n",
    "es = client\n",
    "\n",
    "def create_index():\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"mappings\": {\n",
    "                    \"properties\": {\n",
    "                        \"disease\": {\"type\": \"text\"},\n",
    "                        \"symptoms\": {\"type\": \"text\"},\n",
    "                        \"embedding\": {\n",
    "                            \"type\": \"dense_vector\",\n",
    "                            \"dims\": 384  # Size of the embedding vector, depends on your model\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Index {index_name} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4a851edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "47b2a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Change to the model of your choice\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c09907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate embeddings for symptoms\n",
    "def generate_embeddings(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n",
    "    return embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "887e2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to index documents (diseases and Combined_Symptoms) with embeddings\n",
    "def index_documents(df):\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    symptoms = df['Combined_Symptoms'].tolist()\n",
    "    embeddings = generate_embeddings(symptoms, model, tokenizer)\n",
    "    \n",
    "    actions = []\n",
    "    for i, (disease, symptom, embedding) in enumerate(zip(df['Disease'], symptoms, embeddings)):\n",
    "        action = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": i,  # Optional, you can set a custom ID or let Elasticsearch auto-generate\n",
    "            \"_source\": {\n",
    "                \"disease\": disease,\n",
    "                \"symptoms\": symptom,\n",
    "                \"embedding\": embedding.tolist()  # Convert to list for JSON serialization\n",
    "            }\n",
    "        }\n",
    "        actions.append(action)\n",
    "\n",
    "    print(actions)\n",
    "    helpers.bulk(es, actions)\n",
    "    print(f\"Indexed {len(df)} documents.\")\n",
    "\n",
    "\n",
    "# Function to index documents (diseases and symptoms) with embeddings\n",
    "def index_documents(df):\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    symptoms = df['Combined_Symptoms'].tolist()\n",
    "    embeddings = generate_embeddings(symptoms, model, tokenizer)\n",
    "    \n",
    "    actions = []\n",
    "    for i, (disease, symptom, embedding) in enumerate(zip(df['Disease'], symptoms, embeddings)):\n",
    "        action = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": i,  # Optional, you can set a custom ID or let Elasticsearch auto-generate\n",
    "            \"_source\": {\n",
    "                \"disease\": disease,\n",
    "                \"symptoms\": symptom,\n",
    "                \"embedding\": embedding.tolist()  # Convert to list for JSON serialization\n",
    "            }\n",
    "        }\n",
    "        actions.append(action)\n",
    "\n",
    "    # Bulk indexing with error handling\n",
    "    try:\n",
    "        response = helpers.bulk(es, actions)\n",
    "        print(f\"Successfully indexed {len(actions)} documents.\")\n",
    "    except helpers.BulkIndexError as e:\n",
    "        print(f\"BulkIndexError: {len(e.errors)} documents failed to index.\")\n",
    "        for error in e.errors:\n",
    "            print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efb89c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 41 documents.\n"
     ]
    }
   ],
   "source": [
    "index_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29aa5cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disease_prediction1'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "83cd7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for symptoms based on user input\n",
    "def search_symptoms(query, top_k=3):\n",
    "    model, tokenizer = load_model()\n",
    "    \n",
    "    # Generate the embedding for the user query\n",
    "    query_embedding = generate_embeddings([query], model, tokenizer)[0]\n",
    "\n",
    "    # Search for the most similar symptoms in Elasticsearch\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}  # Match all documents\n",
    "            },\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                \"params\": {\n",
    "                    \"query_vector\": query_embedding.tolist()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Perform the search and get the top-k results\n",
    "    response = es.search(index=index_name, body={\n",
    "        \"size\": top_k,\n",
    "        \"query\": script_query\n",
    "    })\n",
    "\n",
    "    # Parse the response\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        score = hit['_score']\n",
    "        disease = hit['_source']['disease']\n",
    "        symptoms = hit['_source']['symptoms']\n",
    "        results.append({\"disease\": disease, \"symptoms\": symptoms, \"score\": score})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7332673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease: Common Cold\n",
      "Symptoms: Common Cold,  continuous_sneezing,  chills,  fatigue,  cough,  high_fever,  headache,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  chills,  fatigue,  cough,  high_fever,  headache,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  fatigue,  cough,  high_fever,  headache,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  chills,  cough,  high_fever,  headache,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  chills,  fatigue,  high_fever,  headache,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  chills,  fatigue,  cough,  headache,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  chills,  fatigue,  cough,  high_fever,  swelled_lymph_nodes,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  chills,  fatigue,  cough,  high_fever,  headache,  malaise,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain, Common Cold,  continuous_sneezing,  chills,  fatigue,  cough,  high_fever,  headache,  swelled_lymph_nodes,  phlegm,  throat_irritation,  redness_of_eyes,  sinus_pressure,  runny_nose,  congestion,  chest_pain,  loss_of_smell,  muscle_pain\n",
      "Relevance Score: 1.3834\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create_index()\n",
    "\n",
    "# User input for symptom search\n",
    "user_query = \"fever and sore throat\"\n",
    "top_k = 1  # Top 3 most relevant results\n",
    "\n",
    "results = search_symptoms(user_query, top_k)\n",
    "\n",
    "# Display the search results\n",
    "for result in results:\n",
    "    print(f\"Disease: {result['disease']}\")\n",
    "    print(f\"Symptoms: {result['symptoms']}\")\n",
    "    print(f\"Relevance Score: {result['score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fc45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
